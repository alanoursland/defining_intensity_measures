\section{The Illusion of Intensity}

In many scientific and engineering disciplines, the term \textit{intensity} is used informally to describe the strength or presence of a property. In physics, intensity often refers to measurable quantities such as the amplitude of a wave or the power of a signal. In deep learning, activation magnitudes are frequently interpreted as indicating the “strength” of a feature. However, despite its widespread use, intensity lacks a rigorous mathematical definition. This section explores why intensity is often an illusion—an artifact of how distance-based measures are interpreted rather than a fundamental property.

\subsection{Intensity as a Heuristic Concept}

Unlike well-defined measures such as distance, probability, or energy, intensity is often assumed rather than derived. Consider common uses of intensity:

\begin{itemize}
    \item \textbf{Physics:} The brightness of light is proportional to the square of the wave amplitude.
    \item \textbf{Neural Networks:} The magnitude of an activation is often assumed to represent feature strength.
    \item \textbf{Probability and Classification:} A higher softmax output is interpreted as greater confidence in a class.
\end{itemize}

Each of these cases suggests that intensity is a function of some underlying measure, yet no universal formulation exists. In particular, **the assumption that intensity corresponds to feature strength is misleading**, especially in machine learning.

\subsection{Neural Activations Do Not Represent Absolute Feature Strength}

In deep learning, neurons process information by transforming input data through activation functions. The magnitude of a neuron’s activation is often interpreted as reflecting the presence or strength of a learned feature. However, this interpretation is problematic for several reasons:

\begin{enumerate}
    \item \textbf{Relative, Not Absolute:} Neural networks make decisions based on \textit{relative} activations rather than absolute values. A large activation in isolation does not indicate feature presence unless it is significantly larger than activations of competing neurons.
    \item \textbf{Softmax Normalization:} In classification models, final activations are transformed through a softmax function:
    \[
    p_c = \frac{e^{z_c}}{\sum_{c'} e^{z_{c'}}}
    \]
    where \( z_c \) are the logits (pre-softmax activations). This transformation ensures that class scores are interpreted in a \textit{relative} manner—an individual logit has no inherent meaning without comparison to others.
    \item \textbf{Empirical Evidence from Perturbation Studies:} Studies show that **small adversarial perturbations can drastically change classification output**, even when they cause minimal change in activation magnitudes. This suggests that classification is not driven by feature “intensity” but rather by **how distances between class representations are modified**.
\end{enumerate}

These observations suggest that the perception of intensity as an independent measure is an illusion. Instead, activation values should be interpreted through the lens of distance and competition between classes.

\subsection{The Role of Distance in Classification Decisions}

Consider a simple classification problem where a neural network must distinguish between two classes. A naive intensity-based interpretation suggests that an input belongs to a class if the activation for that class exceeds a certain threshold. However, this approach fails in practical settings:

\begin{itemize}
    \item A small activation does not necessarily mean a feature is absent—only that it is not dominant relative to others.
    \item A large activation is not meaningful unless contrasted against alternative class activations.
    \item If neural networks relied on absolute intensities, classification confidence should be stable under uniform scaling of all activations, yet empirical evidence shows that scaling all activations equally can still affect classification outcomes.
\end{itemize}

Instead of interpreting activation values as absolute feature strength, it is more accurate to view them as **encodings of competitive distances**. That is, a classification decision is made not based on a single feature’s presence, but on its proximity to multiple class prototypes.

\subsection{A Thought Experiment: Bigfoot and the Intensity Fallacy}

To illustrate the illusion of intensity, consider the classic analogy of searching for Bigfoot. Suppose a group of researchers sets out to find evidence of Bigfoot in the forest. They define a set of features that might indicate Bigfoot’s presence, such as large footprints, unusual hair samples, and distorted vocalizations.

A naive intensity-based approach suggests that finding a very large footprint is strong evidence of Bigfoot. However, this reasoning is flawed:

\begin{enumerate}
    \item **Footprint Size Alone Is Not Evidence:** Large footprints could be caused by bears, weather effects, or hoaxes.
    \item **The Strength of Evidence Comes from Competitive Comparison:** The key question is not "how large is this footprint?" but rather "how much more likely is this footprint to belong to Bigfoot than to any other known animal?"
    \item **Classification is Relative, Not Absolute:** Without considering competing explanations, any measure of “Bigfoot-ness” is meaningless.
\end{enumerate}

This parallels neural network classification: an activation’s magnitude only matters in comparison to alternative activations. Just as a single footprint does not prove Bigfoot’s existence, a high activation does not prove that a feature is truly present—it only suggests that the input is closer to one learned representation than another.

\subsection{Transitioning from Intensity to Competitive Distance}

Since intensity lacks an independent mathematical definition, we propose a shift in perspective: rather than viewing intensity as feature presence, it should be understood as a function of **competitive distances**. That is, instead of defining intensity as a direct activation value, it should be derived from the difference between distances to competing class representations:

\[
S_c(x) = f(D_{\neg c}(x) - D_c(x))
\]

where:

\begin{itemize}
    \item \( D_c(x) \) is the distance of input \( x \) to the target class representation.
    \item \( D_{\neg c}(x) = \min_{c' \neq c} D_{c'}(x) \) is the distance to the nearest non-target class.
    \item \( S_c(x) \) is the intensity measure, which should be formulated as a function \( f \) of these relative distances.
\end{itemize}

This redefinition eliminates the illusion of intensity and provides a rigorous foundation for understanding classification confidence. In the next section, we will formalize this framework and show why intensity must be defined in terms of contrastive distance rather than absolute activations.

