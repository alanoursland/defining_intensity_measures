\subsection{Why Softmax Works: A Distance Perspective}

Softmax is the most widely used function for converting neural network outputs into probability distributions for classification tasks. Traditionally, softmax is interpreted as a way to normalize raw activations (logits) into a probability distribution, where larger activations indicate stronger feature presence. However, our contrastive distance framework provides a more fundamental explanation: softmax works because it approximates a transformation of relative class distances.

In this section, we reinterpret softmax as a competitive distance function, demonstrating how it aligns with our theoretical framework.

\subsubsection{Traditional View of Softmax}

In standard classification models, the softmax function is applied to logits \( z_c \) to compute class probabilities:

\[
p_c = \frac{e^{z_c}}{\sum_{c'} e^{z_{c'}}}.
\]

where \( z_c \) is typically an unbounded activation value produced by the last layer of a neural network.

This formulation ensures that:

\begin{itemize}
    \item \( p_c \) is always positive and sums to 1 across all classes.
    \item The largest \( z_c \) receives the highest probability, reinforcing the idea that larger activations correspond to stronger class confidence.
    \item The exponential function magnifies differences between logits, exaggerating small activation differences.
\end{itemize}

However, this interpretation relies on the assumption that logits represent an intrinsic measure of class presence, which, as we have shown, is misleading. Instead, logits can be understood as \textit{contrastive distance functions}.

\subsubsection{Reinterpreting Softmax as a Competitive Distance Function}

Instead of treating logits as absolute indicators of class strength, we define them in terms of contrastive distance:

\[
z_c = -D_c(x) + D_{\neg c}(x).
\]

where:

\begin{itemize}
    \item \( D_c(x) \) is the distance to the prototype of class \( c \).
    \item \( D_{\neg c}(x) = \min_{c' \neq c} D_{c'}(x) \) is the distance to the closest competing class.
\end{itemize}

Substituting this into the softmax equation:

\[
p_c = \frac{e^{-(D_c(x) - D_{\neg c}(x))}}{\sum_{c'} e^{-(D_{c'}(x) - D_{\neg c'}(x))}}.
\]

This formulation provides a geometric justification for softmax:

\begin{itemize}
    \item Softmax probability is maximized when \( D_c(x) \) is significantly smaller than \( D_{\neg c}(x) \), meaning the input is clearly closer to class \( c \) than any alternative.
    \item The exponential function ensures smooth probability transitions, preventing abrupt confidence shifts near decision boundaries.
    \item The denominator normalizes probabilities across all classes, preserving relative ranking among competing distances.
\end{itemize}

\subsubsection{Decision Boundaries in Softmax}

A key property of softmax is that classification is based on which \( p_c \) is largest, which occurs when:

\[
D_c(x) < D_{\neg c}(x).
\]

This aligns with our contrastive distance framework: an input is assigned to the class whose prototype it is closest to. The probability difference between classes reflects the separation between competing distances.

\subsubsection{Softmax Overconfidence and High-Dimensional Scaling}

One common issue with softmax is that it tends to produce overconfident predictions, particularly in high-dimensional feature spaces. This is because the exponential function exaggerates small differences in logits, making even slightly preferred classes appear overwhelmingly likely.

Under our framework, this can be reinterpreted as a consequence of high-dimensional distance scaling:

\begin{itemize}
    \item In high-dimensional spaces, feature distances tend to grow larger due to the \textit{curse of dimensionality}.
    \item Since softmax probabilities are computed using exponentials of these distances, even small relative differences can lead to extreme confidence scores.
    \item This explains why deep learning models often require calibration techniques to avoid overconfidence.
\end{itemize}

A potential solution is to modify the softmax transformation by incorporating explicit distance normalization:

\[
p_c = \frac{e^{-\lambda (D_c(x) - D_{\neg c}(x))}}{\sum_{c'} e^{-\lambda (D_{c'}(x) - D_{\neg c'}(x))}}.
\]

where \( \lambda \) is a scaling factor that adjusts the sensitivity of probability estimates to contrastive distance differences.

\subsubsection{Implications for Model Design}

Understanding softmax as a function of competitive distance suggests several improvements to model architecture and training:

\begin{itemize}
    \item \textbf{Distance-Based Training Objectives:} Instead of training models to maximize raw activation differences, loss functions could be designed to maximize contrastive class separation.
    \item \textbf{Improved Calibration:} Introducing explicit contrastive distance constraints could help prevent overconfidence in probabilistic predictions.
    \item \textbf{Robust Classification:} By explicitly tracking relative distances between competing classes, models may be more resistant to adversarial perturbations that attempt to manipulate activation magnitudes.
\end{itemize}

\subsubsection{Conclusion}

Softmax works not because it maps raw activations into probabilities, but because it acts as a transformation of contrastive class distances. This reinterpretation aligns with our framework and provides a theoretical foundation for understanding classification confidence. 

The next section explores how this competitive distance perspective can improve neural network architectures and classification robustness.
